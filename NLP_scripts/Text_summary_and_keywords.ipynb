{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "packed-delay",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niki/anaconda3/lib/python3.8/site-packages/openpyxl/styles/stylesheet.py:221: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "survey_data = pd.read_excel('data/raw_data.xlsx')\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    # This function takes an array of strings and returns an array of cleaned up strings\n",
    "    cleaned_data = []\n",
    "    for row,texts in enumerate(data):\n",
    "        texts = str(texts)\n",
    "        texts = texts.lower()\n",
    "        # remove special characters\n",
    "        texts = texts.replace(r\"(http|@)\\S+\", \"\")\n",
    "        texts = texts.replace(r\"::\", \" \")\n",
    "        texts = texts.replace(r\"’\", \"\")\n",
    "        texts = texts.replace(r\",\", \" \")\n",
    "        texts = texts.replace(r\"[^a-z\\':_]\", \" \")\n",
    "        # remove repetition\n",
    "        #pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "        #texts = texts.replace(pattern, r\"\\1\")\n",
    "        # Transform short negation form\n",
    "        texts = texts.replace(r\"(can't|cannot)\", 'can not')\n",
    "        texts = texts.replace(r\"n't\", ' not')\n",
    "        # Remove stop words\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        stopwords.remove('not')\n",
    "        stopwords.remove('nor')\n",
    "        stopwords.remove('no')\n",
    "        cleaned_line = ''\n",
    "        for word in texts.split(\" \"):\n",
    "            if word not in stopwords:\n",
    "                cleaned_line = cleaned_line + \" \" + word\n",
    "        cleaned_data.append(cleaned_line)\n",
    "    return cleaned_data\n",
    "\n",
    "columns_with_open_responses = ['Q02', 'Q04', 'Q06', 'Q08', 'Q10', 'Q12', 'Q14', 'Q16', 'Q18']\n",
    "data = survey_data[columns_with_open_responses]\n",
    "data = data.reset_index()\n",
    "data = data[1:]\n",
    "data = data.drop(columns = ['index'])\n",
    "all_values = []\n",
    "for column in data:\n",
    "    this_column_values = data[column].tolist()\n",
    "    all_values += this_column_values\n",
    "one_column_df = pd.DataFrame(all_values)\n",
    "one_column_df = one_column_df.dropna()\n",
    "data = one_column_df.reset_index()\n",
    "data = data.drop(columns = ['index'])\n",
    "data = np.array(data)\n",
    "data = clean_data(data)\n",
    "#current_column = 'Q04'\n",
    "#current_data = raw_data[current_column][1:]\n",
    "#current_data = current_data.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "peripheral-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(data):\n",
    "    return_data = []\n",
    "    for line in data:\n",
    "        new_line = \"\"\n",
    "        for word in line.split(\" \"):\n",
    "            if \"\\n\" in word:\n",
    "                new_word = word.replace(\"\\n\",\" \")\n",
    "            else:\n",
    "                new_word = word\n",
    "            new_line = new_line + \" \" + new_word\n",
    "        return_data.append(new_line)\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "higher-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = clean_up(data)\n",
    "all_data_string = \" \".join([i for e in current_data for i in e.split(\" \")])\n",
    "\n",
    "text_summarised = summarize(all_data_string, word_count=200)\n",
    "\n",
    "text_keywords = keywords(all_data_string,split=True, words=20,lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-saint",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "completed-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    if \"   \" in text: text = text.replace(\"   \",\".\")\n",
    "    if \"  \" in text: text = text.replace(\"  \",\".\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "massive-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summarised = split_into_sentences(text_summarised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sound-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(text_summarised,columns=['Text'])\n",
    "keywords_df = pd.DataFrame(text_keywords,columns=['Keywords'])\n",
    "text_df.to_csv('data/output/summarised_text.csv')\n",
    "keywords_df.to_csv('data/output/keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-intellectual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-vertical",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
